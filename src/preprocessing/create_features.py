"""
Script t·∫°o TO√ÄN B·ªò features v√† chu·∫©n h√≥a d·ªØ li·ªáu cho m√¥ h√¨nh LSTM
Tu·∫ßn 3 - Th·ª© 4 + Th·ª© 5

Input: *_clean.csv
Output: 
  - *_features.csv (RSI + MA + Price/Volume/HL + Normalized)
  - *_sequences.npz (X, y cho LSTM)
  - scaler_*.pkl (ƒë·ªÉ inverse transform)

Ch√∫ √Ω:
- Gi·ªØ th√™m c·ªôt `open` cho c·∫£ 5m v√† 1d.
- Khung 5m b·ªè MA_25 (t∆∞∆°ng quan qu√° cao v·ªõi MA_7); khung 1d gi·ªØ ƒë·∫ßy ƒë·ªß.
"""

from __future__ import annotations

import pandas as pd
import numpy as np
from pathlib import Path
import sys
import pickle
from sklearn.preprocessing import MinMaxScaler

# X√°c ƒë·ªãnh project_root theo c·∫•u tr√∫c repo c·ªßa b·∫°n
project_root = Path(__file__).resolve().parent.parent.parent
sys.path.append(str(project_root))

from src.utils.config import config
from src.preprocessing.technical_indicators import TechnicalIndicators


# =============================
#        FEATURE CREATION
# =============================

def create_all_features(df: pd.DataFrame, dataset_name: str) -> pd.DataFrame:
    """
    TH·ª® 4: T·∫°o t·∫•t c·∫£ features cho dataset
    
    Features bao g·ªìm:
    - RSI(14)
    - MA7, MA25
    - Price change %
    - Volume change %
    - High-Low range %
    
    Args:
        df: DataFrame g·ªëc (clean)
        dataset_name: T√™n dataset (ƒë·ªÉ log)
    
    Returns:
        DataFrame v·ªõi t·∫•t c·∫£ features
    """
    print(f"\n{'='*60}")
    print(f"üîß T·∫†O FEATURES: {dataset_name}")
    print(f"{'='*60}")
    
    required = {"datetime", "open", "high", "low", "close", "volume"}
    missing = required - set(df.columns)
    if missing:
        raise ValueError(f"Thi·∫øu c·ªôt b·∫Øt bu·ªôc: {sorted(missing)}")

    df_features = df.copy()
    
    # 1. RSI (14)
    print("[1/5] T√≠nh RSI(14)...")
    df_features = TechnicalIndicators.add_rsi(df_features, period=14, column='close')
    
    # 2. MA (7, 25)
    print("[2/5] T√≠nh MA7, MA25...")
    df_features = TechnicalIndicators.add_moving_averages(df_features, periods=[7, 25], column='close')
    
    # 3. Price Change %
    print("[3/5] T√≠nh Price Change %...")
    df_features['price_change_pct'] = df_features['close'].pct_change() * 100
    
    # 4. Volume Change %
    print("[4/5] T√≠nh Volume Change %...")
    df_features['volume_change_pct'] = df_features['volume'].pct_change() * 100
    df_features['volume_change_pct'].replace([np.inf, -np.inf], np.nan, inplace=True)
    
    # 5. High-Low Range %
    print("[5/5] T√≠nh High-Low Range %...")
    # Tr√°nh chia cho 0 n·∫øu close=0 (hi·∫øm v·ªõi crypto)
    df_features['hl_range_pct'] = ((df_features['high'] - df_features['low']) / df_features['close'].replace(0, np.nan)) * 100
    
    # Ki·ªÉm tra NaN
    print(f"\nKi·ªÉm tra NaN:")
    nan_cols = ['RSI_14', 'MA_7', 'MA_25', 'price_change_pct', 'volume_change_pct', 'hl_range_pct']
    for col in nan_cols:
        nan_count = df_features[col].isna().sum()
        print(f"     ‚Ä¢ {col}: {nan_count} NaN")
    
    # X·ª≠ l√Ω NaN: Forward fill r·ªìi drop ph·∫ßn ƒë·∫ßu c√≤n NaN
    print(f"\nX·ª≠ l√Ω NaN b·∫±ng forward fill + drop ƒë·∫ßu...")
    df_features.fillna(method='ffill', inplace=True)
    rows_before = len(df_features)
    df_features.dropna(inplace=True)
    rows_after = len(df_features)
    
    if rows_before != rows_after:
        print(f"ƒê√£ drop {rows_before - rows_after} d√≤ng ƒë·∫ßu ti√™n c√≥ NaN")
    
    # T·ªïng k·∫øt
    print(f"\nHo√†n th√†nh!")
    print(f"     ‚Ä¢ S·ªë d√≤ng: {len(df_features):,}")
    print(f"     ‚Ä¢ S·ªë features: {len(df_features.columns)}")
    
    return df_features


# =============================
#       FEATURE SELECTION
# =============================

def select_features_for_model(df: pd.DataFrame, timeframe: str) -> pd.DataFrame:
    """
    CH·ªåN FEATURES PH√ô H·ª¢P D·ª∞A TR√äN KHUNG TH·ªúI GIAN
    - 5m: GI·ªÆ `open`, B·ªé `MA_25`
    - 1d: GI·ªÆ `open`, GI·ªÆ `MA_7` + `MA_25`
    """
    print(f"\n{'='*60}")
    print(f"CH·ªåN FEATURES CHO KHUNG {timeframe.upper()}")
    print(f"{'='*60}")
    
    if timeframe == '5m':
        selected_features = [
            'datetime',
            'open', 'close', 'high', 'low', 'volume',  
            'MA_7',              # b·ªè MA_25 v√¨ t∆∞∆°ng quan r·∫•t cao v·ªõi MA_7
            'RSI_14', 'price_change_pct', 'volume_change_pct', 'hl_range_pct'
        ]
        print(f"  ‚Ä¢ B·ªè: MA_25 (t∆∞∆°ng quan r·∫•t cao v·ªõi MA_7)")
        print(f"  ‚Ä¢ S·ªë features (kh√¥ng t√≠nh datetime): 10")
    else:  # '1d'
        selected_features = [
            'datetime',
            'open', 'close', 'high', 'low', 'volume',  
            'MA_7', 'MA_25',  # gi·ªØ c·∫£ 2
            'RSI_14', 'price_change_pct', 'volume_change_pct', 'hl_range_pct'
        ]
        print(f"  ‚Ä¢ Gi·ªØ ƒë·∫ßy ƒë·ªß features")
        print(f"  ‚Ä¢ S·ªë features (kh√¥ng t√≠nh datetime): 11")
    
    # ƒê·∫£m b·∫£o c√°c c·ªôt t·ªìn t·∫°i
    missing = [c for c in selected_features if c not in df.columns]
    if missing:
        raise ValueError(f"Thi·∫øu c·ªôt sau khi t·∫°o features: {missing}")

    df_selected = df[selected_features].copy()
    
    print(f"\nFeatures s·ª≠ d·ª•ng: {[f for f in selected_features if f != 'datetime']}")
    
    return df_selected


# =============================
#          NORMALIZATION
# =============================

def normalize_features(df: pd.DataFrame, dataset_name: str, scaler_dir: Path) -> tuple[pd.DataFrame, MinMaxScaler]:
    """
    TH·ª® 5: Chu·∫©n h√≥a features b·∫±ng MinMaxScaler (scale v·ªÅ [0, 1])
    
    Args:
        df: DataFrame v·ªõi features ƒë√£ ch·ªçn
        dataset_name: T√™n dataset
        scaler_dir: Th∆∞ m·ª•c l∆∞u scaler
    
    Returns:
        tuple: (df_normalized, scaler)
    """
    print(f"\n{'='*60}")
    print(f"CHU·∫®N H√ìA D·ªÆ LI·ªÜU: {dataset_name}")
    print(f"{'='*60}")
    
    df_normalized = df.copy()
    
    # C√°c c·ªôt c·∫ßn normalize (tr·ª´ datetime)
    feature_cols = [col for col in df.columns if col != 'datetime']
    
    print(f"Chu·∫©n h√≥a {len(feature_cols)} features...")
    
    scaler = MinMaxScaler(feature_range=(0, 1))
    df_normalized[feature_cols] = scaler.fit_transform(df[feature_cols])
    
    # L∆∞u scaler
    scaler_filename = f"scaler_{dataset_name.lower().replace(' ', '_')}.pkl"
    scaler_path = scaler_dir / scaler_filename
    with open(scaler_path, 'wb') as f:
        pickle.dump(scaler, f)
    print(f"ƒê√£ l∆∞u scaler: {scaler_path.name}")
    
    # Ki·ªÉm tra k·∫øt qu·∫£
    print(f"\nK·∫øt qu·∫£ normalize:")
    print(f"     ‚Ä¢ T·∫•t c·∫£ features ƒë√£ scale v·ªÅ [0, 1]")
    print(f"     ‚Ä¢ Min: {df_normalized[feature_cols].min().min():.6f}")
    print(f"     ‚Ä¢ Max: {df_normalized[feature_cols].max().max():.6f}")
    
    return df_normalized, scaler


# =============================
#          SEQUENCING
# =============================

def create_sequences(df: pd.DataFrame, window_size: int = 60, dataset_name: str = "") -> tuple[np.ndarray, np.ndarray]:
    """
    TH·ª® 5: T·∫°o sequences cho LSTM
    
    Args:
        df: DataFrame ƒë√£ normalize
        window_size: S·ªë timesteps (m·∫∑c ƒë·ªãnh 60)
        dataset_name: T√™n dataset (ƒë·ªÉ log)
    
    Returns:
        tuple: (X, y) v·ªõi shape ph√π h·ª£p cho LSTM
    """
    print(f"\n{'='*60}")
    print(f"üîß T·∫†O SEQUENCES: {dataset_name} (Window = {window_size})")
    print(f"{'='*60}")
    
    feature_cols = [col for col in df.columns if col != 'datetime']
    data = df[feature_cols].values
    
    X = []
    y = []
    target_idx = feature_cols.index('close')
    
    for i in range(window_size, len(data)):
        X.append(data[i-window_size:i])  # 60 timesteps tr∆∞·ªõc
        y.append(data[i, target_idx])    # Target: close c·ªßa timestep hi·ªán t·∫°i (sau c·ª≠a s·ªï)
    
    X = np.array(X)
    y = np.array(y)
    
    print(f"Ho√†n th√†nh!")
    print(f"     ‚Ä¢ X shape: {X.shape} (samples, timesteps, features)")
    print(f"     ‚Ä¢ y shape: {y.shape} (samples,)")
    print(f"     ‚Ä¢ S·ªë features: {len(feature_cols)} | Target: 'close' @ index {target_idx}")
    print(f"     ‚Ä¢ S·ªë samples: {len(X):,}")
    
    return X, y


# =============================
#              MAIN
# =============================

def main():
    """Main workflow: T·∫°o features + Ch·ªçn l·ªçc + Normalize + Sequences"""
    
    print("\n" + "="*80)
    print("B·∫ÆT ƒê·∫¶U T·∫†O FEATURES + CH·ªåN L·ªåC + CHU·∫®N H√ìA + SEQUENCES")
    print("="*80)
    print("\nWorkflow:")
    print("  [Th·ª© 4] T·∫°o features: RSI, MA, Price/Volume/HL")
    print("  [Th·ª© 4] Ch·ªçn l·ªçc features ph√π h·ª£p (5m b·ªè MA_25; 1d gi·ªØ ƒë·∫ßy ƒë·ªß)")
    print("  [Th·ª© 5] Normalize + T·∫°o sequences cho LSTM")
    
    processed_dir = Path(config.PROCESSED_DATA_DIR)
    scaler_dir = Path(config.MODELS_DIR) / 'scalers'
    scaler_dir.mkdir(parents=True, exist_ok=True)
    
    # Danh s√°ch datasets
    datasets = [
        {
            'name': 'BTC 5m',
            'input': 'BTCUSDT_5m_clean.csv',
            'output': 'BTCUSDT_5m_features.csv',
            'sequences': 'BTCUSDT_5m_sequences.npz',
            'timeframe': '5m'
        },
        {
            'name': 'ETH 5m',
            'input': 'ETHUSDT_5m_clean.csv',
            'output': 'ETHUSDT_5m_features.csv',
            'sequences': 'ETHUSDT_5m_sequences.npz',
            'timeframe': '5m'
        },
        {
            'name': 'BTC 1d',
            'input': 'BTCUSDT_1d_clean.csv',
            'output': 'BTCUSDT_1d_features.csv',
            'sequences': 'BTCUSDT_1d_sequences.npz',
            'timeframe': '1d'
        },
        {
            'name': 'ETH 1d',
            'input': 'ETHUSDT_1d_clean.csv',
            'output': 'ETHUSDT_1d_features.csv',
            'sequences': 'ETHUSDT_1d_sequences.npz',
            'timeframe': '1d'
        }
    ]
    
    success_count = 0
    
    for ds in datasets:
        input_path = processed_dir / ds['input']
        
        # Ki·ªÉm tra file input
        if not input_path.exists():
            print(f"\nL·ªói: Thi·∫øu file {input_path}")
            continue
        
        # Load d·ªØ li·ªáu
        df = pd.read_csv(input_path)
        if 'datetime' not in df.columns:
            raise ValueError("File input c·∫ßn c√≥ c·ªôt 'datetime'")
        df['datetime'] = pd.to_datetime(df['datetime'])
        
        # B∆Ø·ªöC 1: T·∫°o T·∫§T C·∫¢ features (Th·ª© 4)
        df_all_features = create_all_features(df, ds['name'])
        
        # B∆Ø·ªöC 2: CH·ªåN features ph√π h·ª£p (Th·ª© 4)
        df_selected = select_features_for_model(df_all_features, ds['timeframe'])
        
        # B∆Ø·ªöC 3: Normalize (Th·ª© 5)
        df_normalized, scaler = normalize_features(df_selected, ds['name'], scaler_dir)
        
        # L∆∞u features (ƒë√£ normalize v√† ƒë√£ ch·ªçn l·ªçc)
        output_path = processed_dir / ds['output']
        df_normalized.to_csv(output_path, index=False)
        print(f"\nƒê√£ l∆∞u features: {output_path.name} ({len(df_normalized):,} d√≤ng)")
        
        # B∆Ø·ªöC 4: T·∫°o sequences (Th·ª© 5)
        X, y = create_sequences(df_normalized, window_size=60, dataset_name=ds['name'])
        
        # L∆∞u sequences
        sequences_path = processed_dir / ds['sequences']
        np.savez_compressed(sequences_path, X=X, y=y)
        print(f"ƒê√£ l∆∞u sequences: {sequences_path.name}")
        
        success_count += 1
    
    # T·ªïng k·∫øt
    print("\n" + "="*80)
    print("T·ªîNG K·∫æT")
    print("="*80)
    print(f"Th√†nh c√¥ng: {success_count}/{len(datasets)} datasets")
    
    if success_count == len(datasets):
        print("\nHo√†n th√†nh! D·ªØ li·ªáu ƒë√£ s·∫µn s√†ng cho training")
        print(f"\nFiles ƒë√£ t·∫°o:")
        print(f"\n1Ô∏è Features files (CSV - normalized + selected):")
        for ds in datasets:
            print(f"  ‚Ä¢ {ds['output']}")
        
        print(f"\n2Ô∏è Sequences files (NPZ):")
        for ds in datasets:
            print(f"  ‚Ä¢ {ds['sequences']}")
        
        print(f"\n3Ô∏è Scaler files (PKL):")
        for ds in datasets:
            scaler_name = f"scaler_{ds['name'].lower().replace(' ', '_')}.pkl"
            print(f"  ‚Ä¢ {scaler_name}")
        
        # In th√¥ng tin features cu·ªëi c√πng ƒë·ªông theo ƒë√∫ng l·ª±a ch·ªçn ·ªü tr√™n
        f_5m = 10  # open, close, high, low, volume, MA_7, RSI_14, price_change_pct, volume_change_pct, hl_range_pct
        f_1d = 11  # nh∆∞ 5m + MA_25
        print(f"\nFeatures cu·ªëi c√πng:")
        print(f"\n  Khung 5m ({f_5m} features):")
        print(f"    open, close, high, low, volume,")
        print(f"    MA_7, RSI_14,")
        print(f"    price_change_pct, volume_change_pct, hl_range_pct")
        
        print(f"\n  Khung 1d ({f_1d} features):")
        print(f"    open, close, high, low, volume,")
        print(f"    MA_7, MA_25, RSI_14,")
        print(f"    price_change_pct, volume_change_pct, hl_range_pct")

        print(f"\nSequences (window=60):")
        print(f"  ‚Ä¢ X (5m): (samples, 60, {f_5m})")
        print(f"  ‚Ä¢ X (1d): (samples, 60, {f_1d})")
        print(f"  ‚Ä¢ y: (samples,) - target close price")
    
    print("="*80 + "\n")


if __name__ == "__main__":
    main()
